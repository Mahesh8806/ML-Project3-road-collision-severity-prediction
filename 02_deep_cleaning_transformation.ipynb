{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42b138f8",
   "metadata": {},
   "source": [
    "# Notebook 2: Deep Cleaning & Transformation\n",
    "\n",
    "**Project:** Road Collision Severity Prediction  \n",
    "**Section:** D - Deep Cleaning, Weather Interpolation, Geographic Enrichment\n",
    "\n",
    "## What i Will Do in This Notebook:\n",
    "\n",
    "in this notebook, i will take the raw data from notebook 1 and clean it thoroughly. first i need to fix data types like dates and times, remove invalid rows with missing coordinates, and ensure severity values are proper integers.\n",
    "\n",
    "then i will work on the weather data. since the met office data is monthly, i need to interpolate it to daily values so i can match each collision with the weather on that specific day.\n",
    "\n",
    "at the end, i will save everything to clean tables in the database.\n",
    "\n",
    "finally i will enrich the collision data with geographic information by finding the nearest city to each collision location using a kdtree spatial index. this will give us population and distance information that might be useful for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05536a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported and PostgreSQL database connected!\n"
     ]
    }
   ],
   "source": [
    "# importing all libraries i need for data cleaning and transformation\n",
    "# i need pandas for data manipulation, scipy for spatial operations (kdtree)\n",
    "# sqlalchemy for database operations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, text\n",
    "from scipy.spatial import KDTree\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# connecting to postgresql database\n",
    "# loading environment variables from .env file\n",
    "project_directory = Path.cwd()\n",
    "load_dotenv(project_directory / \".env\")\n",
    "\n",
    "# getting postgresql credentials from environment\n",
    "postgres_host = os.getenv(\"POSTGRES_HOST\")\n",
    "postgres_port = os.getenv(\"POSTGRES_PORT\")\n",
    "postgres_database = os.getenv(\"POSTGRES_DB\")\n",
    "postgres_user = os.getenv(\"POSTGRES_USER\")\n",
    "postgres_password = os.getenv(\"POSTGRES_PASSWORD\")\n",
    "\n",
    "# creating connection string and engine\n",
    "database_url = f\"postgresql+psycopg2://{postgres_user}:{postgres_password}@{postgres_host}:{postgres_port}/{postgres_database}\"\n",
    "\n",
    "engine = create_engine(database_url)# above confirms everything loaded properly\n",
    "\n",
    "print(\"libraries imported and postgresql database connected successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dca2f80",
   "metadata": {},
   "source": [
    "## Loading Raw Collision Data\n",
    "\n",
    "now i will load the raw collision data from the database that we inserted in notebook 1. i want to see what columns we have before starting the cleaning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a1268d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 48,472 raw collision records\n",
      "Columns: 44\n",
      "\n",
      "Column names:\n",
      "['collision_index', 'collision_year', 'collision_ref_no', 'location_easting_osgr', 'location_northing_osgr', 'longitude', 'latitude', 'police_force', 'collision_severity', 'number_of_vehicles', 'number_of_casualties', 'date', 'day_of_week', 'time', 'local_authority_district', 'local_authority_ons_district', 'local_authority_highway', 'local_authority_highway_current', 'first_road_class', 'first_road_number', 'road_type', 'speed_limit', 'junction_detail_historic', 'junction_detail', 'junction_control', 'second_road_class', 'second_road_number', 'pedestrian_crossing_human_control_historic', 'pedestrian_crossing_physical_facilities_historic', 'pedestrian_crossing', 'light_conditions', 'weather_conditions', 'road_surface_conditions', 'special_conditions_at_site', 'carriageway_hazards_historic', 'carriageway_hazards', 'urban_or_rural_area', 'did_police_officer_attend_scene_of_accident', 'trunk_road_flag', 'lsoa_of_accident_location', 'enhanced_severity_collision', 'collision_injury_based', 'collision_adjusted_severity_serious', 'collision_adjusted_severity_slight']\n"
     ]
    }
   ],
   "source": [
    "# loading raw collision data from database\n",
    "# this is the data we saved in notebook 1\n",
    "df_raw_collisions = pd.read_sql(\"SELECT * FROM raw_collisions\", engine)\n",
    "print(f\"loaded {len(df_raw_collisions):,} raw collision records from database\")\n",
    "print(f\"number of columns: {df_raw_collisions.shape[1]}\")\n",
    "print(f\"\\nlet me see all the column names:\")\n",
    "\n",
    "print(df_raw_collisions.columns.tolist())# above shows me the structure of the raw data before cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8474fa0",
   "metadata": {},
   "source": [
    "## Cleaning Collision Data\n",
    "\n",
    "now i will start the cleaning process. i need to convert date and time to proper formats, remove rows with invalid coordinates, and make sure severity values are integers. let me work through each cleaning step carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4bd299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning collision data...\n",
      "============================================================\n",
      "Dropped 1 rows with invalid dates/coordinates\n",
      "\n",
      "✓ Cleaned data: 48,471 records\n",
      "Date range: 2025-01-01 00:00:00 to 2025-06-30 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# starting the collision data cleaning process\n",
    "print(\"starting collision data cleaning process\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# first i will create a copy so i dont modify the raw data\n",
    "df_clean_collisions = df_raw_collisions.copy()\n",
    "\n",
    "# converting date column from string to proper datetime format\n",
    "# the dates are in dd/mm/yyyy format\n",
    "df_clean_collisions['date'] = pd.to_datetime(df_clean_collisions['date'], format='%d/%m/%Y', errors='coerce')\n",
    "print(\"converted date column to datetime format\")\n",
    "\n",
    "# converting time column to proper time format\n",
    "# times are in hh:mm format\n",
    "df_clean_collisions['time'] = pd.to_datetime(df_clean_collisions['time'], format='%H:%M', errors='coerce').dt.time\n",
    "print(\"converted time column to time format\")\n",
    "\n",
    "# now i need to remove rows with invalid dates or missing coordinates\n",
    "# these rows wont be useful for analysis\n",
    "initial_row_count = len(df_clean_collisions)\n",
    "df_clean_collisions = df_clean_collisions.dropna(subset=['date', 'longitude', 'latitude'])\n",
    "number_of_dropped_rows = initial_row_count - len(df_clean_collisions)\n",
    "print(f\"dropped {number_of_dropped_rows} rows with invalid dates or missing coordinates\")\n",
    "# above removes unusable records\n",
    "\n",
    "# making sure collision severity is integer type\n",
    "\n",
    "df_clean_collisions['collision_severity'] = df_clean_collisions['collision_severity'].astype(int)# above confirms cleaning completed successfully\n",
    "\n",
    "print(\"converted collision_severity to integer\")print(f\"date range in cleaned data: {df_clean_collisions['date'].min()} to {df_clean_collisions['date'].max()}\")\n",
    "\n",
    "print(f\"\\ncleaned collision data: {len(df_clean_collisions):,} records remaining\")\n",
    "\n",
    "# filtering to keep only valid severity values\n",
    "\n",
    "# 1 = fatal, 2 = serious, 3 = slightdf_clean_collisions = df_clean_collisions[df_clean_collisions['collision_severity'].isin([1, 2, 3])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a083b3",
   "metadata": {},
   "source": [
    "## Weather Data Interpolation\n",
    "\n",
    "now i need to work on the weather data. the met office data comes in monthly aggregates, but i need daily values to match with each collision date. i will interpolate the monthly values to create daily weather records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8d1b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 37,049 monthly weather records\n",
      "\n",
      "Interpolating to daily values...\n",
      "✓ Created 1,127,041 daily weather records\n"
     ]
    }
   ],
   "source": [
    "# loading weather data from database\n",
    "df_weather_raw = pd.read_sql(\"SELECT * FROM raw_weather\", engine)\n",
    "print(f\"loaded {len(df_weather_raw):,} monthly weather records from database\")\n",
    "# above confirms weather data loaded\n",
    "\n",
    "# converting weather columns to numeric type\n",
    "# some values might be stored as strings like 'None' that need handling\n",
    "weather_numeric_columns = ['tmax', 'tmin', 'af', 'rain', 'sun']\n",
    "for column in weather_numeric_columns:\n",
    "    df_weather_raw[column] = pd.to_numeric(df_weather_raw[column], errors='coerce')\n",
    "print(f\"converted {len(weather_numeric_columns)} weather columns to numeric type\")\n",
    "# above ensures all weather values are numbers\n",
    "\n",
    "# looping through each monthly weather record to create daily values\n",
    "for _, monthly_row in df_weather_raw.iterrows():\n",
    "    # skipping rows with missing year or month\n",
    "    if pd.isna(monthly_row['year']) or pd.isna(monthly_row['month']):\n",
    "        continue\n",
    "    \n",
    "    year = int(monthly_row['year'])\n",
    "    month = int(monthly_row['month'])\n",
    "    \n",
    "    # calculating how many days are in this month\n",
    "    # i need to handle december differently since next month is january of next year\n",
    "    if month == 12:\n",
    "        next_month_date = pd.Timestamp(year + 1, 1, 1)\n",
    "    else:\n",
    "        next_month_date = pd.Timestamp(year, month + 1, 1)\n",
    "    \n",
    "    number_of_days_in_month = (next_month_date - pd.Timestamp(year, month, 1)).days\n",
    "    \n",
    "    # now i will create a daily record for each day in the month\n",
    "    # for temperature i keep the monthly value (assuming it represents average)\n",
    "    # for rain, frost days, and sun i divide by days in month to get daily average\n",
    "    for day in range(1, number_of_days_in_month + 1):\n",
    "        daily_weather_record = {\n",
    "            'date': pd.Timestamp(year, month, day),\n",
    "            'station': monthly_row['station'],\n",
    "            'tmax': monthly_row['tmax'],  # keeping monthly max temp\n",
    "            'tmin': monthly_row['tmin'],  # keeping monthly min temp\n",
    "            'af': monthly_row['af'] / number_of_days_in_month if pd.notna(monthly_row['af']) else None,\n",
    "            'rain': monthly_row['rain'] / number_of_days_in_month if pd.notna(monthly_row['rain']) else None,\n",
    "            'sun': monthly_row['sun'] / number_of_days_in_month if pd.notna(monthly_row['sun']) else None\n",
    "\n",
    "        }\n",
    "        }# above confirms interpolation completed successfully\n",
    "\n",
    "        daily_weather_records.append(daily_weather_record)\n",
    "        daily_records.append(daily_record)print(f\"successfully created {len(df_weather_daily):,} daily weather records from monthly data\")\n",
    "\n",
    "\n",
    "print(f\"✓ Created {len(df_weather_daily):,} daily weather records\")\n",
    "\n",
    "# creating dataframe from all daily records\n",
    "df_weather_daily = pd.DataFrame(daily_records)df_weather_daily = pd.DataFrame(daily_weather_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226ac233",
   "metadata": {},
   "source": [
    "## Geographic Enrichment with Nearest City\n",
    "\n",
    "now i will enrich each collision with information about the nearest city. this will give us population density and distance to urban centers, which might help predict severity. i will use a kdtree spatial index for efficient nearest neighbor search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d625095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 48,059 city records\n",
      "UK cities: 1365\n",
      "\n",
      "Finding nearest cities...\n",
      "✓ Added nearest city information\n",
      "✓ Added nearest city information\n"
     ]
    }
   ],
   "source": [
    "# loading population and city data from database\n",
    "df_cities_raw = pd.read_sql(\"SELECT * FROM raw_population\", engine)\n",
    "print(f\"loaded {len(df_cities_raw):,} city records from database\")\n",
    "# above confirms city data loaded\n",
    "\n",
    "# filtering to keep only uk cities since our collision data is from uk\n",
    "df_uk_cities = df_cities_raw[df_cities_raw['country'] == 'United Kingdom'].copy()\n",
    "print(f\"filtered to {len(df_uk_cities)} cities in the united kingdom\")\n",
    "# above shows how many uk cities we have for matching\n",
    "\n",
    "# building kdtree spatial index for fast nearest neighbor lookup\n",
    "# kdtree makes it efficient to find closest city to each collision\n",
    "if len(df_uk_cities) > 0:\n",
    "    # extracting city coordinates for kdtree\n",
    "    city_coordinate_array = df_uk_cities[['lat', 'lng']].values\n",
    "    city_spatial_tree = KDTree(city_coordinate_array)\n",
    "    print(\"built kdtree spatial index for city coordinates\")\n",
    "    \n",
    "    # finding nearest city for each collision using kdtree\n",
    "    print(\"\\nfinding nearest city for each collision...\")\n",
    "    collision_coordinate_array = df_clean_collisions[['latitude', 'longitude']].values\n",
    "    distances_to_cities, city_indices = city_spatial_tree.query(collision_coordinate_array)\n",
    "    \n",
    "    # adding nearest city information to collision dataframe\n",
    "    # i am adding city name, distance in km, and population\n",
    "    df_clean_collisions['nearest_city'] = df_uk_cities.iloc[city_indices]['city_ascii'].values\n",
    "    df_clean_collisions['distance_to_city_km'] = distances_to_cities * 111  # converting degrees to km (rough approximation)\n",
    "    df_clean_collisions['nearest_city_population'] = df_uk_cities.iloc[city_indices]['population'].values\n",
    "    \n",
    "\n",
    "    print(f\"successfully added nearest city information to {len(df_clean_collisions):,} collision records\")    df_clean_collisions['nearest_city_population'] = None\n",
    "\n",
    "    print(f\"average distance to nearest city: {df_clean_collisions['distance_to_city_km'].mean():.2f} km\")    df_clean_collisions['distance_to_city_km'] = None\n",
    "\n",
    "    # above confirms geographic enrichment completed    df_clean_collisions['nearest_city'] = 'Unknown'\n",
    "\n",
    "else:    # if no cities found, adding default values\n",
    "    print(\"warning: no uk cities found in population data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab2e2dc",
   "metadata": {},
   "source": [
    "## Saving Clean Data to Database\n",
    "\n",
    "now i will save both the cleaned collision data and the daily weather data to the database. these clean tables will be used in the next notebooks for analysis and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fc6f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving clean data to database...\n",
      "============================================================\n",
      "✓ Saved 48,471 rows to clean_collisions\n",
      "✓ Saved 48,471 rows to clean_collisions\n",
      "✓ Saved 1,127,041 rows to weather_daily\n",
      "\n",
      "✓ Notebook 2 Complete!\n",
      "✓ Saved 1,127,041 rows to weather_daily\n",
      "\n",
      "✓ Notebook 2 Complete!\n"
     ]
    }
   ],
   "source": [
    "# saving all cleaned data to database\n",
    "print(\"saving cleaned data to postgresql database\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# saving cleaned collision data to clean_collisions table\n",
    "# using chunksize for efficient insertion\n",
    "print(\"saving cleaned collision data...\")\n",
    "df_clean_collisions.to_sql('clean_collisions', engine, if_exists='replace', index=False, chunksize=1000)\n",
    "print(f\"successfully saved {len(df_clean_collisions):,} rows to clean_collisions table\")\n",
    "# above confirms collision data saved\n",
    "\n",
    "\n",
    "# saving daily weather data to weather_daily tableprint(\"\\nnext step: proceed to notebook 3 for exploratory data analysis\")\n",
    "\n",
    "print(\"\\nsaving daily weather data...\")print(f\"- saved clean_collisions and weather_daily tables\")\n",
    "\n",
    "df_weather_daily.to_sql('weather_daily', engine, if_exists='replace', index=False, chunksize=1000)print(f\"- enriched collisions with nearest city information\")\n",
    "\n",
    "print(f\"successfully saved {len(df_weather_daily):,} rows to weather_daily table\")print(f\"- interpolated weather data to {len(df_weather_daily):,} daily records\")\n",
    "\n",
    "# above confirms weather data savedprint(f\"- cleaned {len(df_clean_collisions):,} collision records\")\n",
    "\n",
    "print(\"\\nsummary of what i accomplished:\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)print(\"=\"*60)\n",
    "print(\"notebook 2 complete! all cleaned data saved to database\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
